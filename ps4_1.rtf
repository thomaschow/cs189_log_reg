{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \
mu_0 =\
\
    0.9526\
    0.7311\
    0.7311\
    0.2689\
\
B_1 =\
\
   -0.3868\
    1.4043\
   -2.2842\
\
\
mu_1 =\
\
    0.8731\
    0.8238\
    0.2932\
    0.2198\
\
\
\
B_2 =\
\
   -0.5122\
    1.4527\
   -2.1627\
\
\
2.\
	The negative log likelihood curve for stochastic gradient descent differs from that of batch gradient descent in the sense that it is not as smooth. This is due to the variation in individual data points, that are used to compute the mean and beta vector. At each iteration, a mean and beta are computed based off a new data point, which indicates that the change in the loss is decided by values that may vary drastically. \
\
3. \
	Using a decreasing learning rate improves the loss minimization. The plot indicates that the curve for the loss is much smoother than it was using a constant learning rate (for SGD). This occurs because we are giving less weight to the newly updated parameters as we increase iterations. Thus, each iteration tends to maintain the old parameter values, and the loss changes less. \
\
\
4. We performed 10-fold cross validation and tuned values of lambda as well as tuning the preprocessing methods. We discovered that a lambda of 0.65 and the log transform preprocessing step gives the lowest average error over all the hold-out sets. \
}